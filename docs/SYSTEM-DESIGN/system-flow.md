# System Flow

This document describes Webloom's complete end-to-end execution flow, detailing how data moves through the system, how agents collaborate via message queues, and how a job progresses from creation to analysis.

It includes both high-level and low-level flow diagrams to illustrate the full lifecycle of a scraping job.

## üìå 1. High-Level Overview

The Webloom execution pipeline can be broken down into the following phases:

- Job Creation
- Selector Inference
- Discovery
- Scraping
- Parsing
- Classification
- Change Detection
- Price Tracking
- Storage
- Dashboard Visualization
- Scheduler Auto-Runs

## üìä 2. High-Level Flow Diagram

```
User
 ‚îî‚îÄ> API Gateway
       ‚îî‚îÄ> RabbitMQ (job.start)
               ‚îÇ
               ‚ñº
        Selector Agent
               ‚îÇ
               ‚ñº
         Discovery Agent
               ‚îÇ
               ‚ñº
        URL Scrape Queue
               ‚îÇ
               ‚ñº
         Scraper Agent
               ‚îÇ
               ‚ñº
   Raw HTML ‚Üí Parser Agent
               ‚îÇ
               ‚ñº
       Classifier Agent
               ‚îÇ
               ‚ñº
     Change Detector Agent
               ‚îÇ
               ‚ñº
      Price Tracker Agent
               ‚îÇ
               ‚ñº
         Storage Agent
               ‚îÇ
               ‚ñº
            MongoDB
               ‚îÇ
               ‚ñº
       Dashboard (Next.js)
```

## üîÑ 3. Detailed End-to-End Flow

This section describes EXACTLY what happens from the moment a user creates a job until data appears on the dashboard.

### ‚û§ STEP 1: User Creates a Job

User submits:

- Target URL
- Name
- Schedule (optional)
- Job type
- Extra fields (if provided)

API Gateway performs:

- SSRF validation
- URL normalization
- Quota check
- Job document creation in MongoDB
- Publish message to selectors.request if selectors not provided

### ‚û§ STEP 2: Selector Inference (Heuristic Engine)

Selector Agent receives:

```json
{
  jobId,
  url,
  html
}
```

Agent performs:

- DOM loading (Cheerio)
- Repeating block detection
- Title/price/rating scoring
- CSS selector generation
- Example extraction
- Confidence scoring

Produces:

```json
{
  jobId,
  selectors,
  example,
  confidence
}
```

This is written back into job config in DB.

### ‚û§ STEP 3: Discovery Phase (Extract URLs)

Discovery Agent extracts:

- anchor tags
- pagination links
- product item links
- sub-pages depending on job rules

Discovery outputs URLs to url.to_scrape queue:

```json
{
  jobId,
  url,
  depth,
  parentUrl
}
```

Backpressure logic prevents queue flooding.

### ‚û§ STEP 4: Scraping (Raw HTML Fetching)

Scraper Agent receives URLs and performs:

- SSRF-safe HTTP GET
- User-Agent rotation
- robots.txt optional compliance
- 5s timeout
- 1s domain throttle
- 1 MB max body
- Retry logic (max 3 attempts)

On success, emits to html.raw:

```json
{
  jobId,
  url,
  html,
  status: "success"
}
```

On failure:

- retries
- backoff
- eventual DLQ if fails repeatedly

### ‚û§ STEP 5: Parsing HTML ‚Üí Structured Data

Parser Agent transforms HTML into structured fields:

```json
{
  title,
  description,
  textContent,
  images,
  links,
  metadata,
  priceCandidates,
  ratingCandidates
}
```

Also computes:

- snapshot hash
- normalized text
- version increment

Then publishes to:

html.parsed

### ‚û§ STEP 6: Classification Phase

Classifier Agent determines the page type based on:

- DOM structure
- semantic tags
- price presence
- list repetition
- metadata

Example output:

```json
{
  pageType: "product",
  confidence: 0.87
}
```

Publishes to change.check.

### ‚û§ STEP 7: Change Detection Phase

Change Detector Agent compares the current parsed version to the previous version in DB:

- field-level diffs
- text diff
- image changes
- metadata updates
- structural changes

Produces:

```json
{
  jobId,
  url,
  hasChanges: true/false,
  diffReport
}
```

If changes detected ‚Üí send event to dashboard + notification queue.

### ‚û§ STEP 8: Price Tracking

Price Tracker Agent identifies numeric price patterns and stores:

- old vs new price
- percentage change
- trend direction
- timestamp

Emits price update events to dashboard.

### ‚û§ STEP 9: Storage Agent Writes to DB

Stores all structured data:

- page record
- snapshot
- diff log
- job run updates
- price history

Applies TTL rules based on retention policy.

### ‚û§ STEP 10: Dashboard Updates (Real-Time)

Using SSE (Server-Sent Events), dashboard receives:

- log events
- diff events
- price updates
- job progress updates
- scraper statistics
- worker heartbeats

User sees:

- live scraping progress
- diff visualizations
- price graphs
- version history

### ‚û§ STEP 11: Scheduler Auto-Triggers Recurring Runs

Scheduler Agent runs every minute, evaluating jobs:

If nextRunAt <= now, it:

- creates a new job run
- publishes message to job.start
- enforces free-tier limits

## üß¨ 4. Pipeline with Message Flow Diagram

```
(job.start)
      ‚îÇ
      ‚ñº
(selectors.request)  ‚Üí  selector-agent  ‚Üí (selectors.ready)
      ‚îÇ
      ‚ñº
 discovery-agent  ‚Üí (url.to_scrape)
      ‚îÇ
      ‚ñº
 scraper-agent  ‚Üí (html.raw)
      ‚îÇ
      ‚ñº
 parser-agent  ‚Üí (html.parsed)
      ‚îÇ
      ‚ñº
 classifier-agent  ‚Üí (change.check)
      ‚îÇ
      ‚ñº
 change-detector-agent  ‚Üí (price.update) ‚Üí price-tracker-agent
      ‚îÇ
      ‚ñº
 storage-agent ‚Üí MongoDB
      ‚îÇ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Dashboard
```

## üîí 5. Error, Retry, and Backpressure Flow

```
Scraper Error ‚Üí retry (1) ‚Üí retry (2) ‚Üí retry (3)
       ‚îÇ
       ‚ñº
   Backoff Queue
       ‚îÇ
       ‚ñº
  Failure ‚Üí DLQ ‚Üí Dashboard Alert
```

Backpressure triggers when queues exceed thresholds:

```
If url.to_scrape.length > 200:
      ‚Üí throttle discovery-agent
If html.raw.unacked > 100:
      ‚Üí slow scraper-agent
```

## üìà 6. Job State Machine Flow

```
created ‚Üí running ‚Üí completed
     ‚îÇ         ‚îÇ
     ‚ñº         ‚ñº
   paused ‚Üê stop/cancel
```

Run-level states provide granular insight:

```
queued ‚Üí starting ‚Üí in_progress ‚Üí paused ‚Üí completed ‚Üí failed
```

## üîç 7. Microservice-Level Internal Flow

Each agent follows a standard cycle:
```
connect ‚Üí consume ‚Üí process ‚Üí emit ‚Üí ack
```

If process throws error:

```
nack ‚Üí retry ‚Üí backoff ‚Üí DLQ (if exceeded MAX_RETRIES)
```

## üéØ 8. Complete Flow Summary

From input to insight:

- User configures job
- Selector engine infers selectors
- Discovery finds all pages
- Scraper downloads HTML
- Parser structures content
- Classifier identifies type
- Diff engine checks changes
- Price tracker logs updates
- Storage agent commits to DB
- Dashboard shows live results
- Scheduler repeats cycle

This flow ensures:

- reliability
- scalability
- free-tier safety
- modular maintenance
- real-time insights

END OF FILE