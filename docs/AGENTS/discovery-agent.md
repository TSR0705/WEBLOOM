# Discovery Agent

The Discovery Agent is responsible for identifying all relevant URLs that should be scraped within a job.
It operates immediately after selector inference for listing pages and is a critical component of Webloom's distributed scraping system.

This agent makes Webloom capable of:

- crawling product listings
- navigating paginated pages
- building URL graphs for structured sites
- extracting subpages (reviews, variants, details)
- following internal URLs safely
- expanding scraping coverage automatically

The Discovery Agent is what turns Webloom from a single-page scraper into a multi-page crawler.

## ğŸ¯ Purpose

The Discovery Agent:

- consumes parsed HTML or raw HTML (depending on job phase)
- extracts internal links
- detects pagination
- identifies item URLs
- filters valid URLs
- normalizes URL structure
- publishes URLs to url.to_scrape queue
- respects job depth limits
- prevents infinite crawls

It significantly increases coverage while still remaining free-tier safe.

## ğŸ§© Input Queue
`discovery`

Or triggered implicitly after selector inference for listing pages.

Special case: If job type is "single_page" the agent is bypassed.

Example input message:

```json
{
  "jobId": "abc123",
  "url": "https://example.com/category/phones",
  "html": "<html>...</html>",
  "selectors": {
    "item": ".product-card a"
  },
  "depth": 0
}
```

## ğŸ“¤ Output Queue
`url.to_scrape`

Message example:

```json
{
  "jobId": "abc123",
  "url": "https://example.com/product/iphone13",
  "parentUrl": "https://example.com/category/phones",
  "depth": 1
}
```

## ğŸ” URL Discovery Logic

The discovery pipeline contains several steps:

### âœ” 1. Extract All Anchor Tags

Collect all `<a href="">` values:

```html
<a href="/product/5">...</a>
<a href="https://example.com/page/3">Next</a>
```

Filter and normalize them.

### âœ” 2. Resolve Relative URLs

Relative â†’ absolute:

```
/product/5 â†’ https://example.com/product/5
page/3    â†’ https://example.com/category/page/3
```

### âœ” 3. Filter Internal vs External URLs

Only internal URLs are allowed unless the job explicitly enables external crawling.

Internal definition:

```
hostname(url) === hostname(jobRootUrl)
```

### âœ” 4. Pagination Detection

Recognizes:

- "Next", "Prev", "Next Page"
- numeric pagination (1, 2, 3)
- rel="next" / rel="prev" tags
- URL patterns like ?page=2, /page/3, &p=4

Ideal for e-commerce listings and search results.

### âœ” 5. Item URL Discovery

Uses selector-based extraction for items:

```css
.product-card a  â†’  extract href
.item-link       â†’  extract href
```

Only applied if selectors.item is defined.

### âœ” 6. Depth Control

Enforces:

```
if depth >= job.maxDepth â†’ skip URL
```

Defaults:

- maxDepth = 2 for free-tier
- unlimited depth for paid tiers (future)

### âœ” 7. Deduplication

Prevents duplicate scraping:

- Tracks visited URLs in-memory
- Skips already-enqueued URLs
- Logs duplicates for metrics

### âœ” 8. Free-Tier URL Cap

Stops publishing if:

```
totalUrlsInJob >= MAX_URLS_PER_JOB
```

Default cap = 100 URLs per job.

## ğŸ”„ Workflow
```
receive discovery request
       â”‚
       â–¼
parse HTML or use parsed data
       â”‚
       â–¼
extract all <a> tags
       â”‚
       â–¼
resolve relative URLs
       â”‚
       â–¼
filter internal only
       â”‚
       â–¼
detect pagination links
       â”‚
       â–¼
extract item URLs (if selectors provided)
       â”‚
       â–¼
apply depth control
       â”‚
       â–¼
deduplicate
       â”‚
       â–¼
respect URL cap
       â”‚
       â–¼
publish to url.to_scrape
```

## ğŸ›¡ Safety Mechanisms

### SSRF Protection
- Reject localhost/internal IPs
- Validate hostname matches root domain
- Block file:// and non-http schemes

### Infinite Loop Prevention
- Track visited URLs
- Enforce depth limits
- Cap total URLs per job

### Free-Tier Guardrails
- Max 100 URLs per job
- Max depth = 2
- No external domains
- Rate-limited publishing

## ğŸ” Retries & DLQ Handling

| Failure | Action |
|---------|--------|
| HTML parsing error | retry (3x) |
| URL resolution error | skip |
| Queue full | exponential backoff |
| All retries fail | DLQ |

## âš™ï¸ Environment Variables

```
RABBIT_URL=amqp://guest:guest@rabbitmq:5672
INPUT_QUEUE=discovery
OUTPUT_QUEUE=url.to_scrape
MAX_DEPTH=2
MAX_URLS_PER_JOB=100
```

## ğŸ“‰ Performance Characteristics

- Memory usage: ~25â€“35MB
- Fast link extraction using Cheerio
- Streaming processing
- Minimal CPU overhead
- Efficient deduplication using Set

## ğŸ§ª Testing Strategy

### Unit Tests
- URL resolution logic
- Pagination detection
- Depth enforcement
- Deduplication
- Free-tier caps

### Integration Tests
- Full discovery flow
- Queue publishing
- MongoDB job updates
- Cross-agent coordination

### Edge Cases
- Malformed URLs
- Circular pagination
- Mixed internal/external links
- Empty or missing selectors

## ğŸ“ Summary

The Discovery Agent expands Webloom's reach beyond single pages, enabling:

- intelligent crawling
- pagination traversal
- itemized scraping
- safe link following
- depth-controlled exploration

It is essential for turning Webloom into a true web monitoring platform rather than a simple scraper.

END OF FILE